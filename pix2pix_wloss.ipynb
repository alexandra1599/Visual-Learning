{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08f86f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.8 (you have 1.4.7). Upgrade using: pip install --upgrade albumentations\n",
      "/opt/conda/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from generator import Generator\n",
    "from discriminator import Discriminator\n",
    "import configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af4e1199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape :- torch.Size([5, 3, 256, 256])\n",
      "Y Shape :- torch.Size([5, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "# Data augmentations and transformations\n",
    "both_transform = A.Compose(\n",
    "    [A.Resize(width=256, height=256),], additional_targets={\"image0\": \"image\"},\n",
    ")\n",
    "\n",
    "transform_only_input = A.Compose(\n",
    "    [\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.ColorJitter(p=0.2),\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_only_mask = A.Compose(\n",
    "    [\n",
    "        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], max_pixel_value=255.0,),\n",
    "        ToTensorV2(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Satellite2Map_Data(Dataset):\n",
    "    def __init__(self,root):\n",
    "        self.root = root\n",
    "        list_files = os.listdir(self.root)\n",
    "        #### Removing '.ipynb_checkpoints' from the list\n",
    "        #list_files.remove('.ipynb_checkpoints')\n",
    "        self.n_samples = list_files\n",
    "        \n",
    "            \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.n_samples)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        try:\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "            image_name = self.n_samples[idx]\n",
    "            #print(self.n_samples)\n",
    "            image_path = os.path.join(self.root,image_name)\n",
    "            image = np.asarray(Image.open(image_path).convert('RGB'))\n",
    "            height, width,_ = image.shape\n",
    "            width_cutoff = width // 2\n",
    "            satellite_image = image[:, :width_cutoff,:]\n",
    "            map_image = image[:, width_cutoff:,:]\n",
    "\n",
    "            augmentations = both_transform(image=satellite_image, image0=map_image)\n",
    "            input_image = augmentations[\"image\"]\n",
    "            target_image = augmentations[\"image0\"]\n",
    "\n",
    "            satellite_image = transform_only_input(image=input_image)[\"image\"]\n",
    "            map_image = transform_only_mask(image=target_image)[\"image\"]\n",
    "\n",
    "            return (satellite_image, map_image)\n",
    "        except:\n",
    "            if torch.is_tensor(idx):\n",
    "                idx = idx.tolist()\n",
    "            image_name = self.n_samples[idx]\n",
    "            #print(self.n_samples)\n",
    "            image_path = os.path.join(self.root,image_name)\n",
    "            print(image_path)\n",
    "            pass\n",
    "    \n",
    "    \n",
    "            \n",
    "if __name__==\"__main__\":\n",
    "    dataset = Satellite2Map_Data(\"facades/facades/train\")\n",
    "    loader = DataLoader(dataset, batch_size=5)\n",
    "    for x,y in loader:\n",
    "        print(\"X Shape :-\",x.shape)\n",
    "        print(\"Y Shape :-\",y.shape)\n",
    "        save_image(x,\"satellite.png\")\n",
    "        save_image(y,\"map.png\")\n",
    "        break            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e13483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving examples and checkpoints\n",
    "def save_some_examples(gen, val_loader, epoch, folder):\n",
    "    x, y = next(iter(val_loader))\n",
    "    x, y = x.to(configurations.DEVICE), y.to(configurations.DEVICE)\n",
    "    gen.eval()\n",
    "    with torch.no_grad():\n",
    "        y_fake = gen(x)\n",
    "        y_fake = y_fake * 0.5 + 0.5  # remove normalization#\n",
    "        save_image(y_fake, folder + f\"/y_gen_{epoch}.png\")\n",
    "        save_image(x * 0.5 + 0.5, folder + f\"/input_{epoch}.png\")\n",
    "        save_image(y * 0.5 + 0.5, folder + f\"/label_{epoch}.png\")\n",
    "    gen.train()\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=configurations.DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "        \n",
    "disc_loss = []\n",
    "gen_loss = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc61503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(gen, disc, train_loader, optim_gen, optim_disc, l1_loss, bce_loss):\n",
    "    loop = tqdm(train_loader)\n",
    "    for idx, (x,y) in enumerate(loop):\n",
    "        x = x.cuda()\n",
    "        y = y.cuda()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        \n",
    "        # Generator produces a fake image(of domain y) from input domain x\n",
    "        '''y_fake = gen(x)\n",
    "        # Passing real images to discriminator\n",
    "        d_real = disc(x,y)\n",
    "        # Calculating the bce loss and classifying all real images as 1\n",
    "        d_real_loss = bce_loss(d_real, torch.ones_like(d_real))\n",
    "        # Feeding generator's fake images to the discriminator\n",
    "        d_fake = disc(x,y_fake.detach())\n",
    "        # Calculating the bce loss and classifying all fake images as 0\n",
    "        d_fake_loss = bce_loss(d_fake, torch.zeros_like(d_fake))\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        \n",
    "        disc.zero_grad()\n",
    "        disc_loss.append(d_loss.item())\n",
    "        d_loss.backward()\n",
    "        optim_disc.step()'''\n",
    "        \n",
    "        # Train Discriminator (Critic)\n",
    "        for _ in range(configurations.CRITIC_ITERATIONS):\n",
    "            y_fake = gen(x)\n",
    "            d_real = disc(x, y)\n",
    "            d_fake = disc(x, y_fake.detach())\n",
    "            d_loss = -(torch.mean(d_real) - torch.mean(d_fake))\n",
    "\n",
    "            disc.zero_grad()\n",
    "            disc_loss.append(d_loss.item())\n",
    "            d_loss.backward()\n",
    "            optim_disc.step()\n",
    "\n",
    "            # Weight clipping\n",
    "            for p in disc.parameters():\n",
    "                p.data.clamp_(-configurations.CLIP_VALUE, configurations.CLIP_VALUE)\n",
    "        \n",
    "        # Train Generator\n",
    "        # Pass the generated image to discriminator\n",
    "        d_fake = disc(x, y_fake)\n",
    "        # We want the discriminator to classify them as real\n",
    "        #g_fake_loss = bce_loss(d_fake, torch.ones_like(d_fake))\n",
    "        g_fake_loss = -torch.mean(d_fake)\n",
    "        l1_loss_term = l1_loss(y_fake,y) * configurations.L1_LAMBDA\n",
    "        g_loss = g_fake_loss + l1_loss_term\n",
    "        optim_gen.zero_grad()\n",
    "        gen_loss.append(g_loss.item())\n",
    "        g_loss.backward()\n",
    "        optim_gen.step()\n",
    "        \n",
    "        if idx % 10 == 0:\n",
    "            loop.set_postfix(\n",
    "                d_real=torch.sigmoid(d_real).mean().item(),\n",
    "                d_fake=torch.sigmoid(d_fake).mean().item(),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddb5ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    disc = Discriminator(in_channels=3).cuda()\n",
    "    gen = Generator(in_channels=3).cuda()\n",
    "    optim_disc = torch.optim.Adam(disc.parameters(),lr=configurations.LEARNING_RATE,betas=(configurations.BETA1,0.999))\n",
    "    optim_gen = torch.optim.Adam(gen.parameters(),lr=configurations.LEARNING_RATE,betas=(configurations.BETA1,0.999))\n",
    "    bce_loss = nn.BCEWithLogitsLoss()\n",
    "    lsgan_loss = nn.MSELoss()\n",
    "    l1_loss = nn.L1Loss()\n",
    "    if configurations.LOAD_MODEL:\n",
    "        load_checkpoint(\n",
    "            configurations.CHECKPOINT_GEN,gen,optim_gen,configurations.LEARNING_RATE\n",
    "        )\n",
    "        load_checkpoint(\n",
    "            configurations.CHECKPOINT_DISC,disc,optim_disc,configurations.LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "    train_dataset = Satellite2Map_Data(root=configurations.TRAIN_DIR)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=configurations.BATCH_SIZE,\n",
    "                              shuffle=True,num_workers=configurations.NUM_WORKERS,pin_memory=True)\n",
    "    \n",
    "    val_dataset = Satellite2Map_Data(root=configurations.VAL_DIR)\n",
    "    val_loader = DataLoader(val_dataset,batch_size=1,\n",
    "                        shuffle=True,num_workers=configurations.NUM_WORKERS,pin_memory=True)\n",
    "    \n",
    "    for epoch in range(configurations.NUM_EPOCHS):\n",
    "        train(\n",
    "            gen, disc, train_loader, optim_gen, optim_disc, l1_loss, bce_loss\n",
    "        )\n",
    "        \n",
    "        if configurations.SAVE_MODEL and epoch%50==0:\n",
    "            print(\"Epoch: \",epoch)\n",
    "            save_checkpoint(gen, optim_gen, filename=configurations.CHECKPOINT_GEN)\n",
    "            save_checkpoint(gen, optim_disc, filename=configurations.CHECKPOINT_DISC)\n",
    "        if epoch%2==0:\n",
    "            save_some_examples(gen, val_loader, epoch, folder=\"eval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9949e0ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9107cb97d7f248319178f2849134b5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c820d368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print(configurations.NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e0bf149",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9282f9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
